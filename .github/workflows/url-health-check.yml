name: Dataset URL Health Check

on:
  schedule:
    # Every 10 minutes on the 1st-2nd of each month (cycle window)
    - cron: '*/10 * 1-2 * *'
  workflow_dispatch:
    inputs:
      action:
        description: 'Start new check cycle or check status'
        type: choice
        options:
          - start
          - status
        default: start

permissions:
  issues: write

jobs:
  check-urls:
    runs-on: ubuntu-latest
    name: Check URLs (One State at a Time)

    steps:
      - uses: actions/checkout@v4

      - name: Restore progress from cache
        id: cache-restore
        uses: actions/cache/restore@v4
        with:
          path: /tmp/url_check_progress.json
          key: url-check-never-exact-match
          restore-keys: url-check-

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install requests

      - name: Check next state
        id: checker
        env:
          ACTION: ${{ inputs.action || 'auto' }}
        run: |
          python3 << 'SCRIPT'
          import json
          import os
          import sys
          import requests
          from pathlib import Path
          from datetime import datetime
          from concurrent.futures import ThreadPoolExecutor, as_completed

          PROGRESS_FILE = "/tmp/url_check_progress.json"

          # ── Load existing progress (if any) ──
          progress = {}
          if os.path.exists(PROGRESS_FILE):
              try:
                  with open(PROGRESS_FILE) as f:
                      progress = json.load(f)
              except:
                  progress = {}

          action = os.environ.get("ACTION", "auto")
          today = datetime.utcnow()

          # ── Status check ──
          if action == "status":
              if progress.get("active"):
                  idx = progress.get("current_index", 0)
                  total = progress.get("total_dirs", "?")
                  print(f"Active cycle: {idx}/{total} directories checked")
                  print(f"Dead URLs so far: {len(progress.get('dead_urls', []))}")
                  print(f"Alive so far: {progress.get('alive_count', 0)}")
              else:
                  print("No active cycle.")
              sys.exit(0)

          # ── Determine whether to start a new cycle ──
          is_first_of_month = today.day == 1 and today.hour < 1
          should_start = (action == "start") or (is_first_of_month and not progress.get("active"))

          if should_start and not progress.get("active"):
              progress = {
                  "active": True,
                  "started_at": today.isoformat() + "Z",
                  "current_index": 0,
                  "dirs_checked": [],
                  "dead_urls": [],
                  "alive_count": 0,
                  "total_checked": 0,
              }
              print("Starting new URL check cycle")

          # ── No active cycle? Exit fast ──
          if not progress.get("active"):
              print("No active check cycle. Nothing to do.")
              # Signal no work done
              with open(os.environ.get("GITHUB_OUTPUT", "/dev/null"), "a") as f:
                  f.write("did_work=false\n")
                  f.write("cycle_complete=false\n")
              sys.exit(0)

          # ── Build list of directories to check ──
          data_dir = Path("data")
          check_dirs = []

          # State chain directories
          states_dir = data_dir / "states_chains"
          if states_dir.exists():
              for d in sorted(states_dir.iterdir()):
                  if d.is_dir():
                      check_dirs.append(str(d))

          # Legal planes
          legal_dir = data_dir / "legal_planes"
          if legal_dir.exists():
              check_dirs.append(str(legal_dir))

          # CCDF chains
          ccdf_dir = data_dir / "ccdf_chains"
          if ccdf_dir.exists():
              check_dirs.append(str(ccdf_dir))

          # Reference data
          ref_dir = data_dir / "reference"
          if ref_dir.exists():
              check_dirs.append(str(ref_dir))

          # Sources
          src_dir = data_dir / "sources"
          if src_dir.exists():
              check_dirs.append(str(src_dir))

          progress["total_dirs"] = len(check_dirs)
          idx = progress.get("current_index", 0)

          # ── Cycle complete? ──
          if idx >= len(check_dirs):
              progress["active"] = False
              progress["completed_at"] = today.isoformat() + "Z"
              with open(PROGRESS_FILE, "w") as f:
                  json.dump(progress, f, indent=2)
              with open(os.environ.get("GITHUB_OUTPUT", "/dev/null"), "a") as f:
                  f.write("did_work=true\n")
                  f.write("cycle_complete=true\n")
              print(f"Cycle complete! All {len(check_dirs)} directories checked.")
              print(f"Total URLs: {progress.get('total_checked', 0)}")
              print(f"Alive: {progress.get('alive_count', 0)}")
              print(f"Dead: {len(progress.get('dead_urls', []))}")
              sys.exit(0)

          # ── Process the next directory ──
          target_dir = check_dirs[idx]
          target_name = Path(target_dir).name
          print(f"Checking directory {idx + 1}/{len(check_dirs)}: {target_name}")

          # Extract URLs
          def extract_urls(obj, path=""):
              urls = []
              if isinstance(obj, dict):
                  for k, v in obj.items():
                      if isinstance(v, str) and v.startswith("http"):
                          urls.append((v, f"{path}.{k}"))
                      else:
                          urls.extend(extract_urls(v, f"{path}.{k}"))
              elif isinstance(obj, list):
                  for i, item in enumerate(obj):
                      urls.extend(extract_urls(item, f"{path}[{i}]"))
              return urls

          def check_url(url_info):
              url, field, filepath = url_info
              try:
                  resp = requests.head(url, timeout=15, allow_redirects=True,
                                       headers={"User-Agent": "PMC-HealthCheck/1.0"})
                  return (url, field, filepath, resp.status_code, resp.status_code < 400)
              except requests.exceptions.Timeout:
                  return (url, field, filepath, 0, False)
              except:
                  return (url, field, filepath, -1, False)

          all_urls = []
          seen = set()
          scan_path = Path(target_dir)

          for json_file in scan_path.rglob("*.json"):
              try:
                  with open(json_file) as f:
                      data = json.load(f)
                  for url, field in extract_urls(data):
                      if url not in seen:
                          seen.add(url)
                          all_urls.append((url, field, str(json_file)))
              except:
                  pass

          print(f"  Found {len(all_urls)} unique URLs in {target_name}")

          # Check URLs (3 workers — gentle on servers)
          dead = []
          alive = 0

          if all_urls:
              with ThreadPoolExecutor(max_workers=3) as executor:
                  futures = {executor.submit(check_url, u): u for u in all_urls}
                  for future in as_completed(futures):
                      url, field, filepath, status, ok = future.result()
                      if ok:
                          alive += 1
                      else:
                          dead.append({"url": url, "field": field, "file": filepath, "status": status})

          print(f"  Results: {alive} alive, {len(dead)} dead")

          # Update progress
          progress["dirs_checked"].append(target_name)
          progress["current_index"] = idx + 1
          progress["dead_urls"].extend(dead)
          progress["alive_count"] = progress.get("alive_count", 0) + alive
          progress["total_checked"] = progress.get("total_checked", 0) + len(all_urls)

          with open(PROGRESS_FILE, "w") as f:
              json.dump(progress, f, indent=2)

          with open(os.environ.get("GITHUB_OUTPUT", "/dev/null"), "a") as f:
              f.write("did_work=true\n")
              f.write("cycle_complete=false\n")

          remaining = len(check_dirs) - (idx + 1)
          print(f"  Progress saved. {remaining} directories remaining.")
          SCRIPT

      - name: Save progress to cache
        if: steps.checker.outputs.did_work == 'true'
        uses: actions/cache/save@v4
        with:
          path: /tmp/url_check_progress.json
          key: url-check-${{ github.run_id }}

      - name: Create issue if cycle complete
        if: steps.checker.outputs.cycle_complete == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          PROGRESS="/tmp/url_check_progress.json"

          DEAD_COUNT=$(python3 -c "import json; print(len(json.load(open('$PROGRESS'))['dead_urls']))")
          TOTAL=$(python3 -c "import json; print(json.load(open('$PROGRESS'))['total_checked'])")
          ALIVE=$(python3 -c "import json; print(json.load(open('$PROGRESS'))['alive_count'])")
          DIRS=$(python3 -c "import json; print(len(json.load(open('$PROGRESS'))['dirs_checked']))")

          if [ "$DEAD_COUNT" = "0" ]; then
            echo "All $TOTAL URLs across $DIRS directories are healthy!"
            exit 0
          fi

          # Build dead URL list
          DEAD_LIST=$(python3 -c "
          import json
          progress = json.load(open('$PROGRESS'))
          for d in progress['dead_urls'][:50]:
              status = f'HTTP {d[\"status\"]}' if d['status'] > 0 else 'TIMEOUT' if d['status'] == 0 else 'ERROR'
              print(f'- [{status}] \`{d[\"file\"]}\`')
              print(f'  URL: {d[\"url\"]}')
          if len(progress['dead_urls']) > 50:
              print(f'- ... and {len(progress[\"dead_urls\"]) - 50} more')
          ")

          # Ensure labels exist
          gh label create "url-health" --color "0e8a16" --description "URL health check results" 2>/dev/null || true
          gh label create "maintenance" --color "c5def5" --description "Maintenance tasks" 2>/dev/null || true

          gh issue create \
            --title "URL Health Check: ${DEAD_COUNT} dead URLs found ($(date +%B\ %Y))" \
            --label "url-health,maintenance" \
            --body "$(cat <<EOF
          ## Monthly URL Health Check Report

          **Date:** $(date -u +"%Y-%m-%d")
          **Directories scanned:** ${DIRS}
          **Total URLs checked:** ${TOTAL}
          **Alive:** ${ALIVE}
          **Dead/Broken:** ${DEAD_COUNT}

          *Checked one state at a time with 10-minute intervals between each.*

          ### Dead URLs

          ${DEAD_LIST}

          ### How to fix
          1. Find the dead URL in the listed file
          2. Search for the current URL of that statute/resource
          3. Update the \`source_url\` field
          4. Submit a PR

          ---
          *Auto-generated by PMC URL Health Check*
          EOF
          )"

          echo "Issue created for ${DEAD_COUNT} dead URLs"
